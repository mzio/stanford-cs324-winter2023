---
layout: default
title: Topics / Syllabus
nav_order: 3
has_children: false
---

# Class Topics (Winter 2023)

As a reading resource, please find a list of class topics and relevant materials below. Under each topic, we include a list of related readings, including lecture notes, blog posts, papers, and other resources.

We broadly organize class topics under three areas: [Fundamentals](#fundamentals), [Survey of Existing FMs and their Applications](#survey-of-existing-fms-and-their-applications), and [Societal Considerations & Impact](#societal-considerations--impact)

Note that these topics are *not in order* of when they'll be covered in class. Special thanks to the original content creators, including course notes from a [past version of CS 324](https://stanford-cs324.github.io/winter2022/lectures/).

---

#### Table of Contents  
- [**Fundamentals**](#fundamentals)  
  - [What are foundation models (FMs) and why are they interesting?](#what-are-foundation-models-fms-and-why-are-they-interesting)  
  - [How does *data* impact FMs and what are the downstream effects?](#how-does-data-impact-fms-and-what-are-the-downstream-effects)  
  - [How do we *train* FMs and what are the downstream effects?](#how-do-we-train-fms-and-what-are-the-downstream-effects)  
  - [Model Architectures and Training Objectives for FMs](#model-architectures-and-training-objectives-for-fms)  
  - [Emergent Behaviors and Capabilities](#emergent-behaviors-and-capabilities)  
- [**Survey of Existing FMs and their Applications**](#survey-of-existing-fms-and-their-applications)  
- [**Societal Considerations & Impact**](#societal-considerations--impact)  

---

## Fundamentals 

### What are foundation models (FMs) and why are they interesting?   
* *Course Notes*:  
  - [Large Language Model (LLM) Capabilities](https://stanford-cs324.github.io/winter2022/lectures/capabilities/)  

* *Blog Posts*:
  - [How Foundation Models Changed Our Work](https://hazyresearch.stanford.edu/blog/2022-11-16-whatsup)

* *Papers*:  
  - [Language Models are Few Shot Learners](https://arxiv.org/abs/2005.14165)  
  - [On the Opportunities and Risks of Foundation Models](https://arxiv.org/abs/2108.07258)  

### How does *data* impact FMs and what are the downstream effects?
* *Course Notes*:  
  - [Data for Training FMs](https://stanford-cs324.github.io/winter2022/lectures/data/)
  - [Harms From Data (Part 1)](https://stanford-cs324.github.io/winter2022/lectures/harms-1/)
  - [Harms From Data (Part 2)](https://stanford-cs324.github.io/winter2022/lectures/harms-2/)

* *Blog Posts*:
  - [Foundation Models are Entering their Data-Centric Era](https://hazyresearch.stanford.edu/blog/2022-10-11-datacentric-fms)

* *Papers*:  
  - [The Pile: An 800GB Dataset of Diverse Text for Language Modeling](https://arxiv.org/abs/2101.00027)
  - [Deduplicating Training Data Makes Language Models Better](https://arxiv.org/abs/2107.06499)
  - [Scaling Laws and Interpretability of Learning from Repeated Data](https://arxiv.org/abs/2205.10487)
  - [On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ðŸ¦œ](https://dl-acm-org.stanford.idm.oclc.org/doi/abs/10.1145/3442188.3445922)

### Model Architectures and Training Objectives for FMs  

* *Course Notes*:
  - [LLM Architectures](https://stanford-cs324.github.io/winter2022/lectures/modeling/)  
  - [LLM Training Objectives](https://stanford-cs324.github.io/winter2022/lectures/training/)

* *Blog Posts*:

* *Papers*: 
  - [Attention Is All You Need](https://arxiv.org/abs/1706.03762)  
  - [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555)  
  - [Unifying Language Learning Paradigms](https://arxiv.org/abs/2205.05131)  
  - [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)  


### Emergent Behaviors and Capabilities  

* *Course Notes*:  
  - [Scaling Laws](https://stanford-cs324.github.io/winter2022/assets/pdfs/Scaling%20laws%20pdf.pdf)  

* *Blog Posts*:  
  - [How Does In-context Learning Work?](https://ai.stanford.edu/blog/understanding-incontext/)

* *Papers*: 
  - [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)  
  - [Show Your Work: Scratchpads for Intermediate Computation with Language Models](https://arxiv.org/abs/2112.00114)    
  - [Chain of Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)  
  - [Ask Me Anything: A simple strategy for prompting language models](https://arxiv.org/abs/2210.02441)  
  - [Emergent Abilities of Large Language Models](https://arxiv.org/abs/2206.07682)  
  - [Data Distributional Properties Drive Emergent Few-Shot Learning in Transformers](https://arxiv.org/abs/2205.05055)    
  - [An Explanation of In-Context Learning as Implicit Bayesian Inference](https://arxiv.org/abs/2111.02080)  
  - [Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://arxiv.org/abs/2202.12837)


### Adapting FMs to New Tasks, Data Domains, etc.  

* *Course Notes*:
  - [Adapting Large Language Models](https://stanford-cs324.github.io/winter2022/lectures/adaptation/)  

* *Blog Posts*:  

* *Papers*:
  - [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/abs/2110.08207)    
  - [Finetuned Language Models Are Zero-Shot Learners](https://arxiv.org/abs/2109.01652)    
  - [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/abs/2101.00190)    
  - [Training Language Models to Follow Instructions with Human Feedback](https://arxiv.org/abs/2203.02155)  
  - [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691)  


<!-- Include stuff on model editing, LORA too -->


---

## Survey of Existing FMs and their Applications

---

## Societal Considerations & Impact

---